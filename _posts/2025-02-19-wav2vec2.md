---
layout: single
title:  "[paper] wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations"
permalink: paper/wav2vec2
toc: true
toc_sticky: true
categories: 
  - cs231n
---


**wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations**

## wav2vec 2.0

wav2vec 2.0는 음성 input을 latent space 에서 **마스킹**하고 공동으로 학습된 latent representation의 양자화로 정의된 contrastive task를 해결하는 방식으로 이루어 진다.

labeled data를 1시간으로 줄였을 때 100시간 분량의 이전 모델과 비교하여 더욱 뛰어난 성능을 보였다. labeled data가 제한된 상황에서도 음성인식이 가능하다는 점을 보여준다.

self supervised learning 은 unlabeled data를 가지고 학습을 한 후 label data로 finetuning하는 방식으로 주로 사용되어 왔다. 

wav2vec 2.0는 다층 CNN을 사용하여 음성 오디오를 인코딩하고 latent speech representation의 일부를 마스킹하는 방식으로 이루어진다.


<br>   


### Model

![fig02](https://zerojsh00.github.io/assets/img/2022-07-31-Wav2Vec2/fig02.png)



#### Feature encoder (X->Z)

raw waveform (X) 는 Encoder를 통해 latent speech representation(Z) 가 된다. Encoder 는 temporal convolution과 layer normalization 그리고 GELU 활성화 함수를 포함하는 여러 블록으로 구성된다.

원시 음성 신호 sequence 입력값 X를 입력 받아서 매 T 시점마다 `latent speech representation`인 z1,…,zTz1,…,zT를 출력한다. latent speech representation는 두가지 갈래로 나누어져 다음 단계에서 입력값으로 사용된다. 



#### Contextualized Representations with Transformers(Z->C)

Feature encoder 의 출력 latent speech representation은 Transformer 아키텍처로 구성된 context network를 지난다.

절대 위치 정보를 인코딩하는 fixed positional embeddings 대신 상대적 relative positional embedding 역할을 수행하는 합성곱 층을 사용한다.



#### Quantization module(Z->Q)

self-supervised learning을 위해 feature encoder의 output, latent speech representation을 양차화 과정을 거친다.

양자화 과정은 multiple codebook으로부터 양자화된 표현을 고르고 그들을 concentrate 시켜주는 과정이다.

<img src="https://blog.kakaocdn.net/dn/MVJxn/btrZBrXVXQB/f1hKBAtsfkm9DDYp4pxHG0/img.png" alt="img" style="zoom: 50%;" />

출처 (https://nongnongai.tistory.com/34)

<br>   

G 개의 codebook과 V 개의 entry 가 주어지면, 각 codebook 에서 하나의 entry를 선택한 후, 이를 연결하여 벡터 e1,...,eG 를 구성한다

<img src="https://blog.kakaocdn.net/dn/Lb5tQ/btrZIavSca0/NjJxnfrW4fRpy9Ps53KKV1/img.png" alt="img" style="zoom: 50%;" />

**하나의 codebook e 안에서 V 개의 음소(codeword) 중 현 시점에서 벡터 zt 와 가장 적절하게 대응될 음소 벡터 codeword 를 이산화 과정을 통해 고른 것이 qt**. 
출처 (https://nongnongai.tistory.com/34)

<br>  



g번째 codebook 에서 v번째 code word 벡터가 선택될 확률을 gumbel softmax라 표현하고 이는 다음과 같다.

![img](https://blog.kakaocdn.net/dn/n1MzA/btrZItPqtRK/IxROkwlbl8tQvmihGAgtRk/img.png)



one-hot encoding의 결과를 가지고 각 codebook 내에서 하나의 codeword를 추출하고 이렇게 뽑힌 G개의 codeword를 concatenate한 후, linear transformation 을 거쳐 qt를 만든다.

<img src="https://blog.kakaocdn.net/dn/H7oX6/btrZIswbeFP/5KXub9oQfZLIAmAWKWxwG0/img.png" alt="img" style="zoom:50%;" />



여기서 **양자화**(quantization) 라는 것은 연속된 아날로그신호를 유한개의 레벨(불연속적)로 구분하고, 각 레벨에 대해 특정 값을 부여하는 것이다.

![fig02](https://zerojsh00.github.io/assets/img/2022-07-31-Wav2Vec2/fig02.png)


<br>   


### Training

**Masking**

feature encoder 의 출력인 latent speech representation을 일부 마스킹한 후 transformer(context network)를 지난다. 전체 시간 중 일정 비율 p 만큼을 중복 없이 무작위로 샘플링하여 시작 인덱스로 선택한 후, 각 선택된 인덱스에서 M개의 consecutive time steps를 마스킹한다.
이때, 마스킹된 구간들은 서로 겹칠 수도 있다.



**Contrastive loss**

Contrastive loss는 마스킹된 시간 t를 중심으로, transformer(context network)의 아웃풋 context representation Ct가 주어졌을 때, K+1 개의 quantized candidate representations 중에서 정답에 해당하는 qt를 구분해내는 역할이다. quantized candidate representations는 K개의 distractor(방해 요소)와 1개의 정답 qt로 이루어져 있다. distracter는 다른 마스킹된 시간 단계에서 무작위 샘플링한 거짓 정보이다.
<img width="301" alt="Image" src="https://github.com/user-attachments/assets/61aa99bf-1640-462a-866e-76fd7453ac4a" />


<br>   

### **Reference**

https://nongnongai.tistory.com/34
https://zerojsh00.github.io/posts/Wav2Vec2/
https://huggingface.co/docs/transformers/model_doc/wav2vec2
