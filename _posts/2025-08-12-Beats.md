---
layout: single
title:  "[paper] BEATS : Audio Pre-Training with Acoustic Tokenizers"
permalink: paper/beats
toc: true
toc_sticky: true
categories: 
  - paper
---

**BEATS :AudioPre-Training with Acoustic Tokenizers**


이 논문에서는 다른 도메인에서 많이 쓰이는 **discrete label prediction**을 audio SSL 모델에 적용시키려는 시도를 보인다.  

## Background
Speech와 audio processing 분야에서 SSL이 큰 성공을 보이고 있었다. Wav2vec2.0, HuBERT, data2vec 등이 있으며 특히 low-rescource 상황에서도 좋은 성능을 보인다.
하지만 audio는 speech와 다르게 사람의 음성 이외의 다양한 소리가 포함되기 때문에 더욱 어려운 문제가 된다. SS-AST, AudioMAE와 같은 non-speech 에서도 강인한 모델들이 등장했다.
현재 대부분의 Audio SSL 모델들은 reconstruction loss를 사용하고 있는데 language, vision, speech 분야에서는 discrete label prediction을 주로 사용한다.  

Reconstruction loss 는 low-level audio에는 유리하지만 high-level audio의 의미정보를 반영하지 못한다는 단점을 저자는 지적한다.
discrete label prediction이 audio pre-training에서 reconstruction loss보다 유리한 이유를 세가지 소개한다.
1. 생물학적 관점에서 봤을 때, 인간은 low-level의 세부적인 파형이 아닌 high-level 의미정보를 추출하고 clustering 하여 audio를 이해한다. 예를 들어 개가 짖는 소리를 매번 tone이 다르게 들려도 인간은 같은 개 짖는 소리로 이해한다.
2. 모델링 효율 관점에서, reconstruction loss는 관련 없는 정보를 예측하면서 audio model parameter와 pre-training 자원을 낭비한다. 반대로 discrete label prediction은 불필요한 세부정보는 제거하는 semantic-rich token을 제공하여 모델링 효율성을 높인다.
dicrete label prediction을 이용한 audio SSL 사전학습 model은 language, vision, speech와 audio 사전학습 모델을 통합하도록 한다. language, vision, speech, audio 각각의 task에 맞게 설계하는 대신에 통합모델은 여러 task를 아우르는 범용적인 foundation model을 가능하게 한다.

하지만 일반적인 사전학습을 위한 acoustic tokenizer 그대로 사용하기에는 무리가 있는데, 
-	audio가 연속적인 특성을 지니고 같은 음향 이벤트라도 상황에 따라 duration이 다양하다는 점
-	audio는 speech(음성)이 아닌 non-speech 음향이벤트와 환경적인 sound를 포함해 변동성이 매우 크다. 흔히 쓰는 phoneme(음소) 정보추출 speech tokenizer를 사용할 수 없다는 점
때문이다.  
<img width="235" height="136" alt="image" src="https://github.com/user-attachments/assets/b6cd33de-43e2-42d4-9941-eb222be24990" />  



이 문제를 해결하기 위해 저자는 acoustic tokenizer와 audio SSL model이 iteration으로 적용된 BEATS 모델을 제안한다.
Acoustic tokenizer를 사용해 unlabeled audio의 discrete label을 생성한다. 그리고 이를 이용해 audio SSL model를 최적화한다.
첫 iteration에서는 audio SSL model을 학습시키기 위해 acoustic tokenizer로부터 random projection을 얻어 사용한다(cold start). 이때 making과 discrete label prediction 방법을 사용한다.
두번째 iteration부터는 acoustic tokenizer를 pre-trained 혹은 fine-tuned audio SSL model으로부터 의미정보를 distilling하여 학습시킨다.
모델이 Convergence(수렴) 된 이후에는 SSL model은 teacher model로서 행동하여 acoustic tokenizer가 audio 의미정보를 학습하도록 한다. 이는 Knowledge Disillation의 개념이 사용된 것이다.
추가로 audio SSL model에 소량의 supervised data를 사용해 fine tuning을 진행하기도 한다. 이렇게 fine tuned 된 모델을 acoustic tokenizer 학습의 teacher model로 사용한다.
SSL model의 backborn model로 ViT 모델을 사용한다. 또한 acoustic tokenizer가 생성한 discrete label을 통해 SSL을 학습시킬 때 입력 시퀀스의 75%를 masking하고 예측하는 방식을 사용한다.
 
  
<img width="452" height="82" alt="image" src="https://github.com/user-attachments/assets/63474736-e787-4050-91ce-a151f6b239d6" />  


## 3.1 Iterative Audio Pre-training
처음에 input으로 audio clip이 주어지면 mel-spectrogram과 같은 방법 등으로 acoustic feature를 추출한다. 일정 크기의 patch로 나누어 주고 patch sequence X로 flatten 해준다. Audio SSL 학습을 위해 acoustic tokenizer가 X를 patch level discrete label Z로 분할해준다.
Acoustic tokenizer의 학습을 위해 audio SSL model로 patch sequence X를 인코딩 하게하고 output sequence O를 추출한다.
Pre-trained audio SSL model 혹은 fine-tuned audio SSL model을 acoustic tokenizer 학습의 teacher model로서 사용한다.  

## 3.2 Acoustic Tokenizer
Acoustic Tokenizer는 위에서 언급했듯이 각 iteration을 위한 discrete label을 생성하는데 사용된다. 
### Cold start
첫번째 iteration에서는 아직 teacher model(SSL model)이 사용불가하기 때문에 Random-Projection Tokenizer를 적용함으로써 continuous acoustic feature를 discrete label로 만든다. 
Random-Projection Tokenizer는 그림에서 보이다시피 linear projection layer와 몇 개의 codebook embedding으로 이루어져있다. 가중치는 random initialization 후 고정시켜 학습시키지 않는다. Input feature의 각각의 patch는 linear layer로 처음 projected 되고 codebook embedding 중에 가장 가까운 vector을 찾는다.
Input audio X로부터 patch sequence를 만들고 linear layer W를 통해 Wxt로 투영한다. 투영된 벡터 Wxt와 codebook vectors V로 부터의 거리를 계산한다. 그리고 해당 patch의 discrete label을 가장 가까운 vector의 index로 정의한다.
<img width="191" height="45" alt="image" src="https://github.com/user-attachments/assets/fdc4b290-e37f-4d9b-a9e4-a115650b4038" />  



  
<img width="421" height="280" alt="image" src="https://github.com/user-attachments/assets/48358de8-a241-4d1f-8ace-cd79c0cad3e6" />  

### Iteration
두번째 iteration부터는 acoustic tokenizer의 학습을 위해 마지막 iteration의 audio SSL model을 teacher 모델로서 사용한다. 이때 audio SSL model은 pre-trained 혹은 fine-tuned model 둘 다 될 수 있다. 이때의 tokenizer를 Self-distilled tokenizer라고 부른다.
Self-distilled tokenizer는 처음에 input patch를 discrete label로 변환하기 위해서 Transformer 기반의 tokenizer encoder를 사용한다.
Transformer 기반의 tokenizer는 discrete label과 codebook embedding을 input으로하여 teacher 모델의 output을 예측하도록 학습한다.
Knowledge distillation을 사용함으로써, tokenized된 discrete labels은 teacher model로부터 풍부한 의미정보를 갖게 된다. 그리고 input audio의 불필요한 정보를 적게 갖게 된다.
처음에 Input patch X를 Transformer encoder 12 layer에 넣어 encoded 된 vector sequence E를 얻는다. 그리고 E를 구성하는 각각의 vector et에 대해, codebook embedding V에서 가장 가까운 vector vzt를 찾아 양자화한다. 이때 codebook의 활용을 위해 l2 정규화를 진행하여 사용한다.
teacher model의 마지막 layer output을 예측하기 위해 양자화된 vector sequence Eq를 input으로하여 Transformer estimator 3 layer를 사용한다. Vector quantization에서 미분 불가능한 문제를 다루기 위해 straight-through gradients mechanism을 사용한다. 이는 gradient를 quantized된 vector sequence Eq로부터 encoded된 vector sequence E로 그대로 복사하는 것이다.
self-distilled tokenizer의 학습 목적은 tokenizer estimator의 output sequence와 teacher model의 output sequence 사이의 cosine similarity를 정의하는 것이다. 또한 encoded된 vector sequence E와 quantized된 vector sequence Eq 사이의 MSE를 구하는 것이다.
<img width="452" height="44" alt="image" src="https://github.com/user-attachments/assets/3e66bd13-c63e-4663-8296-12a4bef2e31c" />  

여기서 sg는 stop gradient operator이다. 또한 exponential moving average(EMA)를 사용하여 안정적인 tokenizer 학습을 위한 codebook embedding 최적화를 한다.
여기서 EMA는 최신 데이터에 더 많은 가중치를 부여하여 평균을 구하는 방법이다. 최근 데이터 변화를 더 잘 반영하는 방법이다.  

## 3.3 Audio SSL Model
### Backborn
Beats 모델에서는 linear projection layer와 Transforer encoder layer로 구성된 ViT 구조를 backborn network로 사용한다.
Input audio X로부터 patch sequence를 추출하고 이를 linear projection network를 통해 patch embedding E로 변환한다. 그리고 다시 Transformer encode layer들을 통과시켜 encoded된 patch 표현 R를 얻는다. 
Transformer 하단에 convolution based relative position embedding layer를 추가하였고 좀 더 좋은 position information encoding을 위해 gated relative position bias를 추가하였다.  


### Pre-training
저자는 SSL model pretraining을 위한 Masked Audio Modeling을 제안한다. Input acoustic feature를 reconstruct 하기위해 모델을 최적화하는 이전의 audio pretraining 방법과는 다르게, <u>Beats 모델은 acoustic tokenizer들에 의해 생성된 patch level의 discrete label을 예측하는데 최적화된다.</u>
Input으로 patch sequence X와 tokenizer로 부터이에 상응하는 target discrete acoustic label Z가 주어진다. 랜덤하게 input patch의 75%를 masking 하고 이를 M으로 표현한다. Masking 되지 않은 patch sequence Xu를 ViT encoder에 통과 시켜 encoded된 표현 Ru를 얻는다. 마지막으로 masking 되지 않은 patch 표현과 masking된 patch feature를 전부 discrete acoustic label을 예측하기 위한 label predictor에 통과시킨다. 이처럼 masking 되지 않은 patch만 encoder에 통과시키는 방법은 속도 향상과 downstream task(fine-tuning 시의 task)의 성능 향상도 가져올 수 있다.
MAM의 pre-training 의 목표는 masking 되지 않은 patch sequence로부터 masking 된 위치의 정확한 acoustic label의 log-likelihood를 최대화 하는 cross entropy loss를 사용하는 것이다.
 
### Fine-Tunning
Audio SSL model이 fine tunning 하는 동안 label predictor를 무시하고 ViT encoder에 downstream classification task을 위한 task-specific linear classifier를 추가한다. 
처음에 input acoustic feature에spec-augmentation을 적용하여 시간 축, 주파수 축에서 random masking을 하여 split, flat하여 patch sequence X로 만든다.
(참고) spec-augmentation은 간단히 time warping, frequency의 일부 masking, time의 일부maksing하는 방법을 말한다.
Pre training과는 다르게 전체 patch sequence X를 ViT encoder에 통과시키고 encoded된 표현 R을 얻는다. 마지막으로 category probabilities를 계산하기 위한 linear classifier를 사용한다.  
<img width="228" height="20" alt="image" src="https://github.com/user-attachments/assets/bbf066bc-a692-4630-a45e-922da23c4356" />  
단일 label classification task에서는 cross entropy loss를 적용하고 다중 label classification을 위해서는 binary cross entropy를 적용한다.  

## Conclusion
BEATS 모델의 pre-training 타겟은 기존의 reconstruction based audio SSL model보다 잡음에 더 강하고 의미정보에 더 잘 aligned(정렬) 되어 있음을 확인하였다.
이는 Self-Distilled Tokenizer의 효과와, 본 연구에서 제안한 audio pre-training 프레임워크의 우수성을 뒷받침한다.







