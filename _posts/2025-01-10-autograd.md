---
layout: single
title:  "[Study] Auto Differentiation(자동 미분)"
permalink: study/
toc: true
toc_sticky: true
categories: 
  - study
---

## [Study] Auto Differentiation(자동 미분)

파이토치 사용 중에 많이 만나게 되는 코드가 있는데 "requires_grad = True" , "no_grad()" 가 있다. 이러한 코드의 정확한 의미를 알고자 알아보니 파이토치의 torch.autograd 문법을 정확히 이해해야함을 느꼈다. 이번 포스트는 파이토치의 Auto Differentiation(자동미분)에 대한 이야기이다. <br>



### Auto Differentiation(자동미분)

먼저, Auto differentiation이 무엇인가?

loss fn의 값을 줄여나가기 위해 모델의 parameter를 조정하는 작업을 **optimization(최적화)**라고 함을 이전에 알아보았다.

이 optimization에는  gradient decent(경사하강법), SGD, Momentum, Adam 등 다양한 방법이 존재하는데 대부분의 방법은 gradient를 이용한다.

(gradient를 이용하지 않는 optimization이 있나해서 검색해보니 Gradient-free Optimization이라는 개념이 있다고 한다. )

컴퓨터는 gradient를 어떻게 계산할 수 있을까?

크게

- Numeric Differentiation
- Symbolic Differentiation
- Automatic Differentiation (AD)
  - Forward Mode
  - Reverse Mode

로 구분할 수 있다.

<br>

#### Numeric Differentiation

![RockVillains on X: "You read it like this: "F", Prime of "X", is equal to  the Limit as "H" approaches Zero, of the function of "F" of X plus H;  minus, the](data:image/png;base64,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)

Numeric Differentiation는 미분의 정의를 이용하여 gradient를 계산하는 방법이다.

우리가 기울기를 구할 때 사용하는 방법이라고 이해하면 될 것 같다.

간단하고 구현하기 쉬운 방법이지만, 신경망에서 모든 매개변수에 대해 계산하려면 계산 비용이 매우 높아 적용하기는 힘들다.

또한 컴퓨터는 숫자 표현 한계로 근삿값이 표현되는데 이로 인해 아주 작은 값은 0으로 처리 되는 등의 Round-off error 로 신경망에서의 사용이 회피된다.

<br>

#### Symbolic Differentiation

미적분의 규칙을 사용하여, 주어진 수식의 미분값을 구체적인 기호 표현으로 변환하는 체계적인 과정이다.


$$
g(x)=cos(x)+2x−e^x
$$

$$
f(g)=4g^2
$$

$$
f(g(x))=4(cos(x)+2x−ex)^2
$$

$$
\cfrac{df}{dx}
​
 = 
\cfrac{df}{dg}
​
 ⋅ 
\cfrac{dg}{dx}
​
 =8(cos(x)+2x−e^x
 )⋅(−sin(x)+2−e^x
 )
$$

Symbolic Differentiation 은 표현식 팽창 문제로 신경망에서 비효율적으로 적용된다. 예를 들어 곱셈법칙을 이용하면 원래보다 더 많은 항과 계산을 포함하게 되는데 이는 굉장히 많은 연산이 필요한 신경망에서 문제가 더욱 심각해진다.

<br>

#### Automatic Differentiation (AD)

자동 미분은 합성 함수를 구성하는 변수와 기본 연산으로 표현하고, 이러한 연산의 미분을 Chain Rule을 통해 연결하여 전체 함수의 미분값을 계산하는 방법이다.

![img](https://velog.velcdn.com/images/gypsi12/post/fd6bfe7c-6a05-4acc-abaa-7df922d4f571/image.png)

가장 안쪽 함수부터 하나씩 계산하여 새로운 변수로 치환해주는 개념이다.

![img](https://velog.velcdn.com/images/gypsi12/post/d56f9635-d2d9-45d7-96e0-0c683ae2088b/image.png)

우리가 구하고자 하는 미분값은 dy/dx이고 이 값을 이러한 chain들의 미분값들을 활용하여 구하는 방법이다.

여기서 dy/dx를 구할때,

오른쪽 값(dw1/dx) 부터 구하는 방법, 즉 함수의 제일 안쪽부터 순차적으로 구하는 방법을 **Forward Mode**

왼쪽 값(dy/dw2) 부터 구하는 방법, 즉 함수의 제일 바깥쪽부터 역방향으로 구하는 방법을 **Reverse Mode** 라고 한다.

이제부터 아래의 예시를 통해 Forward Mode, Reverse Mode에 대해 알아보겠다.

<br>
$$
y=f(x1,x2)=x1x2+x2−ln(x1)
$$
<img src="https://github.com/andrew-m-holmes/nura/blob/develop/blog/images/comp-graph-dark.png?raw=true" alt="computational graph" style="zoom: 33%;" />

<br>

##### Forward Mode

Forward Mode는 함수의 각 기본 연산에서 다음 두 가지를 동시에 수행한다.

1. **Primal**: 중간 변수 값을 계산.
2. **Tangent**: 각 변수의 미분값을 동기적으로 계산.

Forward Mode는 전체 야코비안 행렬을 계산할 필요 없이 선택한 입력과 벡터만으로 야코비안-벡터 곱을 효율적으로 구할 수 있다.



<img width="675" alt="Screenshot 2025-01-10 at 5 47 03 PM" src="https://github.com/user-attachments/assets/8d47710c-ed63-4ee6-aaba-539a69259c23" />

위에서 본 것 처럼 함수의 제일 안쪽부터 순차적으로 구하는 것을 볼 수 있다.

<br>

##### Reverse Mode

<img width="726" alt="Screenshot 2025-01-10 at 5 58 29 PM" src="https://github.com/user-attachments/assets/1b57886c-184a-4893-b6a2-25815c5778a3" />


<br>



이렇게 복잡한 연산을 미분할 때 Automatic Differentiation(자동미분)을 활용하면 굉장히 간편함을 알 수 있다.

복잡한 Neural Network에서 이러한 방식으로 gradient를 구하게 된다.

<br>

- Forward mode의 경우

입력 차원이 출력차원보다 매우 작은 경우 효율적이다.



- Reverse mode의 경우에

출력차원이 입력차원보다 매우 작은 경우 효율적이다.

<br>

**Reference**

[huggingface blog (https://huggingface.co/blog/andmholm/what-is-automatic-differentiation)](https://huggingface.co/blog/andmholm/what-is-automatic-differentiation)

