---
layout: single
title:  "[paper] Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift"
permalink: paper/BN
toc: true
toc_sticky: true
categories: 
  - paper
---

다양한 Model architecture들을 보면 batch normalization 이라는 블럭이 자주 보인다. 그래서 batch normalization이 무엇이고 왜 쓰는건가?  
  
이 논문의 내용을 간단하게 요약하자면 이렇다.  
  
딥러닝에서 학습을 진행할 때, 우리는 batch 단위로 학습을 하게 된다. 이 때 각각의 입력, 즉 미니배치 마다 분포가 다르기 때문에 모델이 학습하기 어려워진다.  
이 문제를 해결하기 위해 배치 단위로 normalization(정규화)를 사용한다.
  


# Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift

## Intro

딥러닝에서 많이 쓰이는 **Stochastic Gradient Descent (SGD)**에서는 안정적인 gradient 추정, 그리고 병렬 연산 효율을 위해 mini-batch를 이용하여 학습을 진행한다.  
SGD는 간단하고 효율적인 방법이지만 각 layer의 input의 분포가 바뀌는 것은 문제가 될 수 있는데, 각 layer가 새롭게 바뀌는 분포에 계속해서 적응 해야하기 때문이다.  
즉, 입력 분포가 고정되어 있는 편이 후반부 네트워크 학습에 유리하다는 점을 말하고 있다.

네트워크의 입력 분포가 안정적인 것이 유리한 이유가 하나 더 있다.  
예를 들어 sigmoid 함수를 가지는 한 layer가 있다고 가정하자.

$$
z = g(Wu + b), \quad g(x) = \frac{1}{1 + \exp(-x)}
$$

여기서 학습해야 할 learning parameter인 \(W\)와 \(b\)로부터 \(x\)는 영향을 받는다. 만약 \(x\)가 non-linear (sigmoid) 함수의 포화 영역, 즉 non-linear 가 아닌 부분에 들어가게 되고 이것이 반복되면 기울기 소실 문제를 가지게 된다. 이것이 **vanishing gradient 문제**이다.  
이러한 문제를 다양한 방법으로 해결하고 있지만 입력분포를 안정적으로 유지하면 vanishing gradient 문제를 줄일 수 있다.

위에서 언급한 것처럼 딥러닝 학습에서 내부 노드의 분포가 바뀌는 현상을 **Internal Covariate Shift** 라고 한다. 이를 제거하기 위해서 저자들은 **Batch Normalization**을 제안한다.

크게 세 가지 장점을 언급한다.

1. 파라미터의 크기나 초기 값 설정에 대한 의존을 줄일 수 있어 더 큰 learning rate을 사용해도 divergence(발산) 위험이 적다.  
2. 모델을 normalization을 하여 dropout과 같은 추가적인 regularization의 필요를 줄인다.  
3. Non-linear 함수의 포화영역에 갇히지 않도록 해서 sigmoid 함수와 같은 saturating nonlinearity를 사용할 수 있게 한다.  

---

## 2. Towards Reducing Internal Covariate Shift

앞서 말했듯이 **Internal Covariate Shift**는 딥러닝 학습에서 내부 노드의 분포가 바뀌는 현상이다. 이를 줄이기 위한 방법을 모색한다.

네트워크의 입력이 whitening 되었을 때 학습이 더 빠르게 수렴한다는 사실은 알려져 있다.  
**Whitening**은 입력을 선형적으로 변환하여 평균이 0, 분산이 1이 되도록 하여 서로의 상관관계가 사라지도록 하는 방법을 말한다. 우리가 알고 있는 이런 식이라고 이해하면 되겠다.

$$
Z = \frac{X - m}{\sigma}
$$

매 학습마다 activation(활성값)에 whitening을 하는 방법으로 입력분포를 고정시키는 방법을 생각해낼 수 있다. 하지만 이 방법은 gradient descent에서 파라미터 업데이트 시 정규화를 같이 진행시켜야 하므로 gradient descent의 효과가 줄어든다고 한다.  

따라서 네트워크의 정보를 유지하면서도, 개별 학습 샘플의 activation을 훈련 데이터 전체의 통계량에 정규화하는 방법을 제안한다.

---

## 3. Normalization via Mini-Batch Statistics

각 layer에 전부 whitening을 적용하는 것은 비용이 많이 들고 역전파 시 모든 경우에 미분가능하지 않기에 단순화를 진행해야 한다.

### 1. Scalar feature 독립 정규화
Input과 output의 feature들을 동시에 whitening하는 대신, **각 scalar feature들을 독립적으로 정규화**한다.  

<img width="265" height="117" alt="image" src="https://github.com/user-attachments/assets/1ac0f311-f215-4b3b-a562-6134ed43b743" />  


단순하게 입력을 정규화하게 되면 non-linear 함수의 선형구간에 갇힐 수 있다. 이렇게 되면 non-linear 함수를 사용하는 의미가 사라지게 될 것이다.  
따라서 identity transform을 적용한다. 정규화된 값을 scale하고 shift 하는 방법이다. 아래 그림과 같이 바꿔준다고 이해하면 좋아보인다.  

<img width="951" height="622" alt="image" src="https://github.com/user-attachments/assets/6428f74a-aac9-480b-83aa-caffa9162e34" />  


$$
y^{(k)} = \gamma^{(k)} \hat{x}^{(k)} + \beta^{(k)}
$$

### 2. Mini-batch 기반 정규화
전체 데이터 셋을 사용해서 정규화 하지 않고 **mini-batch 속 각 activation에 대한 평균과 분산의 추정치**로 정규화를 진행한다.  
여기에 사용되는 평균과 분산은 역전파가 가능하고 공분산이 아닌 분산을 사용하기 때문에 더 편리하다.  
<img width="547" height="383" alt="image" src="https://github.com/user-attachments/assets/7a91fd78-6c38-4979-92c7-a6c1e643feeb" />  


---

## 3.1 Training and Inference with Batch-Normalized Networks

- 학습 시에는 activation 정규화가 **mini-batch**에 의존하기 때문에 효율적인 학습이 가능하다.  
- 하지만 추론, 평가 시에는 output이 input에만 전적으로 의존해야 하기 때문에 이 방법을 그대로 사용하는 것은 불가능하다.  
- 따라서 네트워크가 훈련된 이후에는, 정규화를 위해 mini-batch의 통계가 아니라 **전체 데이터(population) 통계**를 사용한다.

$$
\hat{x} = \frac{x - \mathbb{E}[x]}{\sqrt{\operatorname{Var}[x]} + \epsilon}
$$

E[x]와 Var[x]는 mini-batch가 아닌 전체 데이터 통계이다.  
그리고 추론 시 분산은 불편추정량을 사용한다.

$$
\operatorname{Var}[x] = \frac{m}{m-1} \cdot \mathbb{E}_{B}\!\left[\sigma_B^2\right]
$$

m은 mini-batch의 크기, \(\sigma_B^2\)는 각 mini-batch의 샘플 분산이다.  
즉, 추론 시에는 평균과 분산이 고정되어 있기 때문에 정규화는 단순히 activation에 대해 선형변환을 취하는 것과 같다.

---

## 3.2 Batch-Normalized Convolutional Networks

Batch normalization은 **non-linear 함수 바로 앞**에서 진행된다. 즉, 아래의 식에서는 \(Wu+b\)를 정규화하는 것이다. 여기서 \(g\)는 sigmoid와 같은 non-linear 함수이다.

$$
z = g(Wu + b)
$$

\(u\)는 다른 non-linear의 output일텐데 분포의 형태가 크게 변할 수 있어 u를 바로 정규화하지 않는다.  
또한 bias \(b\)는 정규화 과정에서 상쇄되기 때문에 무시한다. 따라서 다음과 같은 식으로 Batch normalization이 진행된다.

$$
z = g(\text{BN}(Wu))
$$

Convolution layer의 경우, convolution의 성질을 지키며 정규화를 진행해야 하기 때문에 같은 feature map 내의 값들이 동일한 방식으로 정규화되어야 한다. 따라서 mini-batch 속 모든 위치의 activation들을 함께 정규화한다.

---

## 3.3 Batch Normalization enables higher learning rates

일반적으로 learning rate가 너무 크면 gradient가 소실되어 local minima에 빠질 수 있다.  
Batch normalization은 네트워크 전반에서 activation들을 정규화하여, parameter의 작은 변화가 activation이나 gradient의 더 큰 변화로 증폭되는 것을 막는다.  
또한 Batch normalization은 학습이 parameter scale에 덜 민감하게 만든다.  
대게 높은 learning rate는 layer parameter의 scale을 증가시키고 이는 역전파에서 gradient를 증폭시켜 모델이 터지게 한다.

---

## 3.4 Batch Normalization regularizes the model

Batch normalization은 한 샘플이 독립적으로 고려되지 않고 같은 mini-batch 내의 다른 샘플들과 정규화되기 때문에 주어진 하나의 샘플이 결정적인 출력을 내지 않게 된다.  
따라서 generalization 성능이 더 좋아짐을 확인했다고 한다.
