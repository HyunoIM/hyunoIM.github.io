---
layout: single
title:  "[CS231n] 4. Introduction to Neural Networks"
categories: cs231n
---



# Introduction to Neural Networks 정리  









앞의 3장에서는 loss function과  optimization에 대해 학습했다.

Optimization은 경사하강법을 이용해서 loss function의 최솟값을 찾아가며 가중치를 업데이트 하는 과정을 의미한다.

하지만 우리가 접하는 대부분의 그래프는 거대하다. 이렇게 각 층의 weight를 늘어놓고 계산하는 것은 불가능에 가깝다.

따라서 이제부터 설명하는 이론을 배우게 된다.  




<img width="782" alt="image" src="https://github.com/JDVVd/deeplearning_with_python/assets/155535882/922141a4-ad87-4a55-8aaf-7ec09a055664">

수식에 대해 연산자를 노드로 표현하는 그래프로 변환한 것, 이런 과정을 computational graph 라고 이 강의에서 설명한다.  





## Backpropagation 역전파

<img width="804" alt="image" src="https://github.com/JDVVd/deeplearning_with_python/assets/155535882/9e9f08d1-da5a-4e8a-a779-aedd6de48eef">


Backpropagation 역전파는 가중치를 업데이트 하는 과정이다.

매개변수 q와 f를 도입한다. 우리가 구하고자 하는 것은 인풋값이 마지막 값에 주는 영향이다. 이를 위해 backward pass를 해야한다. 뒤에서부터 계산이 필요하다. 

Z가 f에 영향을 주는 정도는 df/dz이고 이는 위에 식에서 볼 수 있듯이  q이기 때문에 3이다. Z가 3배만큼 영향력을 주고있다고 할 수 있고 Q는 -4만큼 영향력을 주고있다.

df/dy는 Chainrule을 통해 구한다. -> z x 1 = -4 이고

df/dx = z x 1 = -4 이다.  



<img width="804" alt="image" src="https://github.com/JDVVd/deeplearning_with_python/assets/155535882/47331e67-0c31-404c-8b4a-653d57c63ed9">


dz/dx, dz/dy는 f내부의 input과 output만으로 표현할 수 있는 수식이기 때문에 local gradient라고 한다.

forward pass하는 과정에서 바로 로컬 gradient를 구할 수 있다. local gradient는 forward pass 시에 바로 구해서 메모리에 저장을 해둔다가 핵심이라고 말한다.

global gradient는 backward pass 동안에 구할 수 있다.

이렇게 local gradient와 global gradient를 곱해서 gradient값을 구할 수 있다.

여기서 구한 dL/dx와 dL/dy는 이렇게 또 backward pass가 반복되며 끝까지 진행하게 된다.  



<img width="832" alt="image" src="https://github.com/JDVVd/deeplearning_with_python/assets/155535882/e57583f3-1eb9-4512-a103-3ec843803027">


이 예제의 경우 5개의 인풋이 존재한다. Computational graph로 표현한 것이다.

구하려는 것은 df/w0, df/w1 … 이다. 이들 각각에 대한 최종적인 영향력을 구하고 싶은 것이다.

맨 오른쪽은 df/df로 identity function이므로 gradient는 1이다.

db/da=-1/(1.37)^2, dL/db=1 -> -0.53  



<img width="898" alt="image" src="https://github.com/JDVVd/deeplearning_with_python/assets/155535882/e940fdd3-b962-499f-8c93-c4c637ec751c">


이렇게 동일하게 진행을 쭉 하고 덧셈 연산의 경우 local gradient가 1이 되기 때문에 gradient 값이 그대로 전달된다.  



<img width="924" alt="image" src="https://github.com/JDVVd/deeplearning_with_python/assets/155535882/a7d3af13-e516-4775-9455-e5237b3b94da">


이 예제로 나온 함수가 sigmoid function과 유사한 형태이다.

sigmoid function을 미분하면 위와 같은 형태가 된다. 다른 어떤 것과도 연계가 없이 자기 자신만으로 표현이 되는 것을 볼 수 있다.

예시의 부분을 sigmoid gate라고 한다. 위에서 구한 식을 이용해 sigmoid gate에 해당하는 부분의 계산을 줄일 수 있다.  



<img width="864" alt="image" src="https://github.com/JDVVd/deeplearning_with_python/assets/155535882/d2351682-d095-471e-b0c9-6046f199cce9">



Add gate는 전의 gredient 값을 그대로 전달한다. local gradient값이 1이기 때문이다.

Mul gate는 local gradient가 곱하는 반대쪽 값이 되기 때문에 'switcher' 라고 표현한다.

Max gate는 두 변수 중 더 큰 하나에만 gradient 정보를 전달하기 때문에 'router' 라고 표현한다.  



<img width="826" alt="image" src="https://github.com/JDVVd/deeplearning_with_python/assets/155535882/ebb89354-f33c-4b11-9c28-76af02dba35e">


Backward pass 과정에서 이처럼 뒤쪽이 여러 개인 경우에는 더해주면 된다.  



<img width="878" alt="image" src="https://github.com/JDVVd/deeplearning_with_python/assets/155535882/a5f66a95-ccc2-4378-9b6a-82bc91b35eaf">


앞으로 접하게 될 대부분의 경우 input값이 행렬로 이루어져 있을 것이다. X,Y,Z 가 모두 벡터이다. local gradient도 행렬이 될 것이다. 입력변수에 대한 출력변수의 편미분을 jacobian이라 한다.

Vertorized operation이 일어나게 된다.

 Input이 [4096,1]이고 output이 [4096,1]인 f 라는 연산이 있으면 [input] = [Jacobian] * [output] 이므로 jacobian matrix의 크기는 4096,4096의 크기를 가져야만 성립한다.  




## Neural Network

<img width="957" alt="image" src="https://github.com/JDVVd/deeplearning_with_python/assets/155535882/54e502d0-58a9-41b1-8e20-33505815fb4b">


기존의 스코어 function을 구할때는 f=Wx로 표현했다. Neural network에서는 W1x를 처리한 것을 다시 처리한다. 

위의 예시는 input인 x에서는 3072개를 가지고 w1이라는 가중치를 만나 100개의 노드로 히든레이어를 구성할 것이고 다시 w2 가중치와 연산되어서 output에서는 10개의 클래스로 분류하는 간단한 과정이다.

여서기 Max함수는 activation(활성화) 함수의 예시이다. 활성화함수는 선형 연산을 비선형으로 바꾸어 주는 역할을 한다. 예시에서 w1과 w2의 가중치는 모두 선형이기 때문에 이렇게 레이어를 거친다 해서 성능이 좋아지지 않는다. 따라서 활성화 함수로 비선형성을 만들어준다. 활성화 함수에 대해서는 뒤에서 더 다루도록 한다.  



<img width="1882" alt="image" src="https://github.com/JDVVd/deeplearning_with_python/assets/155535882/90c45327-1144-489b-b4d5-f421418a6970">


Nearest-neighbor의 한계는 다양한 형태의 이미지를 하나로 합쳐서 생각한다는 것이다. 각각의 클래스에는 딱 한 개의 분류가 존재했기 때문이다. 분류가 하나뿐이기 때문에 그림과 같이 왼쪽을 보는 말과 오른쪽을 보는 말 등을 하나의 분류로 만든다.

Neural network에서는 히든 레이어에 예를 들어 100개의 노드가 있고 노드 각각이 하나의 피쳐를 담당한다.

왼쪽을 보는 말, 오른쪽을 보는 말 등이 있다. 이 노드의 수는 실험을 통해 적합한 값을 찾아야 한다.  



<img width="914" alt="image" src="https://github.com/JDVVd/deeplearning_with_python/assets/155535882/387ec40a-cf5b-41aa-adfd-b4426883ce47">



하나의 뉴런이 여러 개가 연결되면서 전달과정이 일어난다. 뇌의 신경은 역치 이상의 자극을 전달하게 되는데 활성화 함수가 그 역할을 하게된다. 합 연산 후에 활성화 함수를 적용해서 비선형 구조로 만들어주고 다음 뉴런으로 전달한다.

하지만 인공신경망이 우리의 뇌와 같다고 하기는 무리이다. 생물학적 뉴런은 종류도 다양하고 더 복잡한 연산을 수행한다. 시냅스도 엄청나게 복잡한 가중치를 가진다. 생물학적 신경망과 비교가 사실은 불가하고 이해를 위한 예시라고 생각하면 될 것 같다.  



<img width="857" alt="image" src="https://github.com/JDVVd/deeplearning_with_python/assets/155535882/62d294d9-1cdb-494b-bc7a-a2a8cc496734">


활성화 함수 중 시그모이드 함수를 써왔다.

X가 0일 때 0.5를 가지고 0~1사이 값을 가지기 때문에 특정 뉴런의 영향력을 특정해주기 쉽기 때문이다.

 

2012년에 탄생한 ReLU 함수는 0과 x값 중 큰 값을 선택한다. 가장 디폴트로 사용되는 활성화함수이다.

RELU 함수를 변형한 Leaky ReLU 등 다양한 Activation 함수가 있다.  



<img width="798" alt="image" src="https://github.com/JDVVd/deeplearning_with_python/assets/155535882/58906f60-eaba-4a31-badb-83690ee56333">



Input layer는 레이어 수에 제외하여 표현한다.

각각의 레이어들이 모든 노드들이 연결되어 있으면 Fully-Connexted layer이다. 효율적인 계산을 위해 필요하다.  



<img width="912" alt="image" src="https://github.com/JDVVd/deeplearning_with_python/assets/155535882/9dd36a16-aa19-4060-918b-e15d938c3ad0">


뉴런의 수에 따른 분류를 하는 능력 비교한 것이다. 뉴런의 수가 많을수록 분류를 잘한다.

Neural network의 사이즈가 regualaization 역할을 하는 것은 아니다.

Regualaization은 overfitting 되지 않게 모델의 복잡도를 낮추는 것을 의미한다.

일반화를 하기 위해서는 Regularization strength를 더 높여야한다.

뉴럴 네트워크는 Regularization을 잘한다는 전제로 더 크면 클수록 좋다.

