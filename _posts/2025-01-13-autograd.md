---
layout: single
title:  "[PyTorch] Auto Differentiation with torch.autograd"
permalink: pytorch/
toc: true
toc_sticky: true
categories: 
  - pytorch
---

앞서 auto differentiation에 대해 알아보았다. [(참고)](https://jdvvd.github.io/study/)

이제는 pytorch 에서 auto differentiation이 어떻게 작동하는지 알아본다.

매우 간단한 신경망을 먼저 정의 해보겠다.



```python
import torch

x = torch.ones(7)  # input tensor
y = torch.zeros(4)  # expected output
w = torch.randn(7, 4, requires_grad=True)
b = torch.randn(4, requires_grad=True)
z = torch.matmul(x, w)+b
loss = torch.nn.functional.binary_cross_entropy_with_logits(z, y)
```

![img](https://tutorials.pytorch.kr/_images/comp-graph.png)

이 예시에서 w 와 b는 최적화를 해야하는 매개변수이다. 따라서 이 매개변수의 gradient를 계산해야 한다.

w = torch.randn(7, 4, requires_grad=True)
b = torch.randn(4, requires_grad=True)

와 같이 해당 매개변수에 requires_grad=True를 적용한 모습을 볼 수 있다.



```python
loss.backward()
print(w.grad)
print(b.grad)
```

w와 b의 gradient를 계산하기 위해 loss.backward()를 호출하고 w.grad 와 b.grad로 각각의 값을 가져온다.<br>

`requires_grad=True`인 모든 텐서들은 연산 기록을 추적하고 변화도 계산을 지원한다. 만약 모델 학습이 끝나고 단순 적용할 때나 순전파 연산만 필요한 경우에는 torch.no_grad() 로 연산 추적을 멈출 수 있다.

```python
z = torch.matmul(x, w)+b
print(z.requires_grad)

with torch.no_grad():
    z = torch.matmul(x, w)+b
print(z.requires_grad)
```
